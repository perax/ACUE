}
# Chunk 6
param <- f.desc_grad(x, y, 1, 1, norma = 1E-4)
# Chunk 7
modelo <- lm(y ~ x)
summary(modelo)
# Chunk 8
egbeta0  <- NULL
egbeta1  <- NULL
elmbeta0 <- NULL
elmbeta1 <- NULL
for (i in 1:100) {
# Generamos la muestra
n <- 100
x <- runif(n, min = 0, max = 5) # x_i: n puntos aleatorios en el intervalo [min,max]
beta0 <- 2 # Parámetro beta0 del modelo
beta1 <- 5 # Parámetro beta1 del modelo
epsilon <- rnorm(n, sd = 1) # error (con desviación típica sd=1)
y <- beta0 + beta1 * x + epsilon # y_i
# metodos
param  <- f.desc_grad(x, y, 1, 1, norma = 0.0001)
modelo <- lm(y ~ x)
# errores
egbeta0 <- c( egbeta0, abs(param$sol[1]-2)/2 )
egbeta1 <- c( egbeta1, abs(param$sol[2]-5)/5 )
elmbeta0 <- c(elmbeta0, abs(modelo$coefficients[1]-2)/2 )
elmbeta1 <- c(elmbeta1, abs(modelo$coefficients[2]-5)/5 )
}
hist(egbeta0, breaks = 20, font.main = 1, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
# descenso gradiente
etix <- paste("ER(mean=", round(mean(egbeta0), 3),
", sd=", round(sd(egbeta0), 3), ")", sep = "")
hist(egbeta0, breaks = 20, font.main = 1, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 12, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 2, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 3, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 4, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 5, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 6, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 7, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
hist(egbeta0, breaks = 20, font.main = 1, main = "beta0 con desc. grad.", xlab = etix,
ylab = "Frecuencia")
f.desc_grad(x,y,1,1)
a <- f.desc_grad(x,y,1,1)
# funcion sigmoidal
f <- function(x){
return(1/(1+exp(-x)))
}
# derivada de sigmoidal
df <- function(x){
fx <- f(x)
return(fx*(1-fx))
}
activacion <- function(x = 0, w1, w2, b1, b2){
z2 <- w1%*%x + b1
a2 <- f(z2)
z3 <- w2%*%a2 + b2
a3 <- f(z3)
return(list(z2 = z2, z3 = z3, a2 = a2,  a3= a3))
}
delta <- function(y = 0, w2, a){
d3 <- (y - a$a3)*df(a$z3)
d2 <- (t(w2)%*%d3)*df(a$z2)
return(list(d2 = d2, d3 = d3))
}
# probamos que están bien definidas
x <- seq(-10, 10, 0.01)
plot(x,  f(x), type = 'l')
plot(x, df(x), type = 'l')
# datos
alfa <- 0.5
lambda <- 1
x  <- matrix(c(1, 1), nrow = 2, ncol = 1)
y  <- matrix(1, nrow = 1, ncol = 1)
w1 <- matrix(c(-2, 1, 1, -1, 3, -1), nrow = 3, ncol = 2, byrow = T)
w2 <- matrix(c(2, 3, 1), nrow = 1, ncol = 3)
b1 <- matrix(c(0, -1, 1), nrow = 3, ncol = 1)
b2 <- matrix(0, nrow = 1, ncol = 1)
# esto nos muestra los valores de z2, z3, a2, a3, además a1 = x.
cat('Los valores de z2, z3, a2, a3 se muestran a continuación. Además, a1 = x = (1, 1).')
a <- activacion(x, w1, w2, b1, b2)
a
# esto nos muestra los valores de los delta(i)
cat('Los valores de los delta(i) son:')
d <- delta(y = 1, w2, a)
d
# algoritmo del gradiente para el backpropagation
pw1 <- matrix(0, nrow = 3, ncol = 2)
pw2 <- matrix(0, nrow = 1, ncol = 3)
pb1 <- matrix(0, nrow = 3, ncol = 1)
pb2 <- matrix(0, nrow = 1, ncol = 1)
n <- 1
for (i in 1:n) {
a <- activacion(x, w1, w2, b1, b2)
d <- delta(y = 1, w2, a)
# derivadas
dw1 <- d$d2%*%t(x)
dw2 <- d$d3%*%t(a$a2)
db1 <- d$d2
db2 <- d$d3
# pasos
pw1 <- pw1 + dw1
pw2 <- pw2 + dw2
pb1 <- pb1 + db1
pb2 <- pb2 + db2
# actualizamos pesos
w1 <- w1 - alfa*(pw1 + lambda*w1)
w2 <- w2 - alfa*(pw2 + lambda*w2)
b1 <- b1 - alfa*pb1
b2 <- b2 - alfa*pb2
}
cat('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
# funcion sigmoidal
f <- function(x){
return(1/(1+exp(-x)))
}
# derivada de sigmoidal
df <- function(x){
fx <- f(x)
return(fx*(1-fx))
}
# función identidad
id <- function(x){
return(x);
}
# derivada de la identidad
did <- function(x){
return(1);
}
activacion <- function(x = 0, w1, w2, b1, b2){
z2 <- w1%*%x + b1
a2 <- f(z2)
z3 <- w2%*%a2 + b2
a3 <- id(z3)
return(list(z2 = z2, z3 = z3, a2 = a2,  a3= a3))
}
delta <- function(y = 0, w2, a){
d3 <- (y - a$a3)*did(a$z3)
d2 <- (t(w2)%*%d3)*df(a$z2)
return(list(d2 = d2, d3 = d3))
}
# probamos que están bien definidas
x <- seq(-10, 10, 0.01)
plot(x,  f(x), type = 'l')
plot(x, df(x), type = 'l')
# datos
alfa <- 0.5
lambda <- 1
x  <- matrix(c(1, 1), nrow = 2, ncol = 1)
y  <- matrix(1, nrow = 1, ncol = 1)
w1 <- matrix(c(-2, 1, 1, -1, 3, -1), nrow = 3, ncol = 2, byrow = T)
w2 <- matrix(c(2, 3, 1), nrow = 1, ncol = 3)
b1 <- matrix(c(0, -1, 1), nrow = 3, ncol = 1)
b2 <- matrix(0, nrow = 1, ncol = 1)
# esto nos muestra los valores de z2, z3, a2, a3, además a1 = x.
cat('Los valores de z2, z3, a2, a3 se muestran a continuación. Además, a1 = x = (1, 1).')
a <- activacion(x, w1, w2, b1, b2)
a
# esto nos muestra los valores de los delta(i)
cat('Los valores de los delta(i) son:')
d <- delta(y = 1, w2, a)
d
# algoritmo del gradiente para el backpropagation
pw1 <- matrix(0, nrow = 3, ncol = 2)
pw2 <- matrix(0, nrow = 1, ncol = 3)
pb1 <- matrix(0, nrow = 3, ncol = 1)
pb2 <- matrix(0, nrow = 1, ncol = 1)
n <- 1
for (i in 1:n) {
a <- activacion(x, w1, w2, b1, b2)
d <- delta(y = 1, w2, a)
# derivadas
dw1 <- d$d2%*%t(x)
dw2 <- d$d3%*%t(a$a2)
db1 <- d$d2
db2 <- d$d3
# pasos
pw1 <- pw1 + dw1
pw2 <- pw2 + dw2
pb1 <- pb1 + db1
pb2 <- pb2 + db2
# actualizamos pesos
w1 <- w1 - alfa*(pw1 + lambda*w1)
w2 <- w2 - alfa*(pw2 + lambda*w2)
b1 <- b1 - alfa*pb1
b2 <- b2 - alfa*pb2
}
cat('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
# funcion sigmoidal
f <- function(x){
return(1/(1+exp(-x)))
}
# derivada de sigmoidal
df <- function(x){
fx <- f(x)
return(fx*(1-fx))
}
# función identidad
id <- function(x){
return(x);
}
# derivada de la identidad
did <- function(x){
return(1);
}
activacion <- function(x = 0, w1, w2, b1, b2){
z2 <- w1%*%x + b1
a2 <- f(z2)
z3 <- w2%*%a2 + b2
a3 <- id(z3)
return(list(z2 = z2, z3 = z3, a2 = a2,  a3= a3))
}
delta <- function(y = 0, w2, a){
d3 <- - (y - a$a3)*did(a$z3)
d2 <- (t(w2)%*%d3)*df(a$z2)
return(list(d2 = d2, d3 = d3))
}
# probamos que están bien definidas
x <- seq(-10, 10, 0.01)
plot(x,  f(x), type = 'l')
plot(x, df(x), type = 'l')
# datos
alfa <- 0.5
lambda <- 1
x  <- matrix(c(1, 1), nrow = 2, ncol = 1)
y  <- matrix(1, nrow = 1, ncol = 1)
w1 <- matrix(c(-2, 1, 1, -1, 3, -1), nrow = 3, ncol = 2, byrow = T)
w2 <- matrix(c(2, 3, 1), nrow = 1, ncol = 3)
b1 <- matrix(c(0, -1, 1), nrow = 3, ncol = 1)
b2 <- matrix(0, nrow = 1, ncol = 1)
# esto nos muestra los valores de z2, z3, a2, a3, además a1 = x.
cat('Los valores de z2, z3, a2, a3 se muestran a continuación. Además, a1 = x = (1, 1).')
a <- activacion(x, w1, w2, b1, b2)
a
# esto nos muestra los valores de los delta(i)
cat('Los valores de los delta(i) son:')
d <- delta(y = 1, w2, a)
d
# algoritmo del gradiente para el backpropagation
pw1 <- matrix(0, nrow = 3, ncol = 2)
pw2 <- matrix(0, nrow = 1, ncol = 3)
pb1 <- matrix(0, nrow = 3, ncol = 1)
pb2 <- matrix(0, nrow = 1, ncol = 1)
n <- 1
for (i in 1:n) {
a <- activacion(x, w1, w2, b1, b2)
d <- delta(y = 1, w2, a)
# derivadas
dw1 <- d$d2%*%t(x)
dw2 <- d$d3%*%t(a$a2)
db1 <- d$d2
db2 <- d$d3
# pasos
pw1 <- pw1 + dw1
pw2 <- pw2 + dw2
pb1 <- pb1 + db1
pb2 <- pb2 + db2
# actualizamos pesos
w1 <- w1 - alfa*(pw1 + lambda*w1)
w2 <- w2 - alfa*(pw2 + lambda*w2)
b1 <- b1 - alfa*pb1
b2 <- b2 - alfa*pb2
}
cat('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
# funcion sigmoidal
f <- function(x){
return(1/(1+exp(-x)))
}
# derivada de sigmoidal
df <- function(x){
fx <- f(x)
return(fx*(1-fx))
}
# función identidad
id <- function(x){
return(x);
}
# derivada de la identidad
did <- function(x){
return(1);
}
activacion <- function(x = 0, w1, w2, b1, b2){
z2 <- w1%*%x + b1
a2 <- f(z2)
z3 <- w2%*%a2 + b2
a3 <- id(z3)
return(list(z2 = z2, z3 = z3, a2 = a2,  a3= a3))
}
delta <- function(y = 0, w2, a){
d3 <- - (y - a$a3)*did(a$z3)
d2 <- (t(w2)%*%d3)*df(a$z2)
return(list(d2 = d2, d3 = d3))
}
# probamos que están bien definidas
x <- seq(-10, 10, 0.01)
plot(x,  f(x), type = 'l')
plot(x, df(x), type = 'l')
# datos
alfa <- 0.5
lambda <- 1
x  <- matrix(c(1, 1), nrow = 2, ncol = 1)
y  <- matrix(1, nrow = 1, ncol = 1)
w1 <- matrix(c(-2, 1, 1, -1, 3, -1), nrow = 3, ncol = 2, byrow = T)
w2 <- matrix(c(2, 3, 1), nrow = 1, ncol = 3)
b1 <- matrix(c(0, -1, 1), nrow = 3, ncol = 1)
b2 <- matrix(0, nrow = 1, ncol = 1)
# esto nos muestra los valores de z2, z3, a2, a3, además a1 = x.
cat('Los valores de z2, z3, a2, a3 se muestran a continuación. Además, a1 = x = (1, 1).')
a <- activacion(x, w1, w2, b1, b2)
a
# esto nos muestra los valores de los delta(i)
cat('Los valores de los delta(i) son:')
d <- delta(y = 1, w2, a)
d
# algoritmo del gradiente para el backpropagation
pw1 <- matrix(0, nrow = 3, ncol = 2)
pw2 <- matrix(0, nrow = 1, ncol = 3)
pb1 <- matrix(0, nrow = 3, ncol = 1)
pb2 <- matrix(0, nrow = 1, ncol = 1)
n <- 1
for (i in 1:n) {
a <- activacion(x, w1, w2, b1, b2)
d <- delta(y = 1, w2, a)
# derivadas
dw1 <- d$d2%*%t(x)
dw2 <- d$d3%*%t(a$a2)
db1 <- d$d2
db2 <- d$d3
# pasos
pw1 <- pw1 + dw1
pw2 <- pw2 + dw2
pb1 <- pb1 + db1
pb2 <- pb2 + db2
# actualizamos pesos
w1 <- w1 - alfa*(pw1 + lambda*w1)
w2 <- w2 - alfa*(pw2 + lambda*w2)
b1 <- b1 - alfa*pb1
b2 <- b2 - alfa*pb2
}
cat('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
# funcion sigmoidal
f <- function(x){
return(1/(1+exp(-x)))
}
# derivada de sigmoidal
df <- function(x){
fx <- f(x)
return(fx*(1-fx))
}
# función identidad
id <- function(x){
return(x);
}
# derivada de la identidad
did <- function(x){
return(1);
}
activacion <- function(x = 0, w1, w2, b1, b2){
z2 <- w1%*%x + b1
a2 <- f(z2)
z3 <- w2%*%a2 + b2
a3 <- id(z3)
return(list(z2 = z2, z3 = z3, a2 = a2,  a3= a3))
}
delta <- function(y = 0, w2, a){
d3 <- - (y - a$a3)*did(a$z3)
d2 <- (t(w2)%*%d3)*df(a$z2)
return(list(d2 = d2, d3 = d3))
}
# probamos que están bien definidas
x <- seq(-10, 10, 0.01)
plot(x,  f(x), type = 'l')
plot(x, df(x), type = 'l')
# datos
alfa <- 0.5
lambda <- 1
x  <- matrix(c(1, 1), nrow = 2, ncol = 1)
y  <- matrix(1, nrow = 1, ncol = 1)
w1 <- matrix(c(-2, 1, 1, -1, 3, -1), nrow = 3, ncol = 2, byrow = T)
w2 <- matrix(c(2, 3, 1), nrow = 1, ncol = 3)
b1 <- matrix(c(0, -1, 1), nrow = 3, ncol = 1)
b2 <- matrix(0, nrow = 1, ncol = 1)
# esto nos muestra los valores de z2, z3, a2, a3, además a1 = x.
cat('Los valores de z2, z3, a2, a3 se muestran a continuación. Además, a1 = x = (1, 1).')
a <- activacion(x, w1, w2, b1, b2)
a
# esto nos muestra los valores de los delta(i)
cat('Los valores de los delta(i) son:')
d <- delta(y = 1, w2, a)
d
# algoritmo del gradiente para el backpropagation
pw1 <- matrix(0, nrow = 3, ncol = 2)
pw2 <- matrix(0, nrow = 1, ncol = 3)
pb1 <- matrix(0, nrow = 3, ncol = 1)
pb2 <- matrix(0, nrow = 1, ncol = 1)
n <- 1000
for (i in 1:n) {
a <- activacion(x, w1, w2, b1, b2)
d <- delta(y = 1, w2, a)
# derivadas
dw1 <- d$d2%*%t(x)
dw2 <- d$d3%*%t(a$a2)
db1 <- d$d2
db2 <- d$d3
# pasos
pw1 <- pw1 + dw1
pw2 <- pw2 + dw2
pb1 <- pb1 + db1
pb2 <- pb2 + db2
# actualizamos pesos
w1 <- w1 - alfa*(pw1 + lambda*w1)
w2 <- w2 - alfa*(pw2 + lambda*w2)
b1 <- b1 - alfa*pb1
b2 <- b2 - alfa*pb2
}
cat('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
activacion(x, w1, w2, b1, b2)$a3
cat('La predicción para la salida es: ')
activacion(x, w1, w2, b1, b2)$a3
print('Los resultadados tras', n, 'iteraciones del backpropagation son:')
paste('Los resultadados tras', n, 'iteraciones del backpropagation son:')
paste('La predicción para la salida es: ')
activacion(x, w1, w2, b1, b2)$a3
paste('Los resultadados tras', n, 'iteraciones del backpropagation son:')
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
paste('Los resultadados tras', n, 'iteraciones del backpropagation son:',
list(w1 = w1, b1 = b1, w2 = w2, b2 = b2))
# Carga y manejo de datos
file <- "Telco-Customer-Churn.csv"
df <- read.csv(file = file, header = T, sep = ",", na.strings = "NA")
df$SeniorCitizen[df$SeniorCitizen == 0] <- 'No'
df$SeniorCitizen[df$SeniorCitizen == 1] <- 'Yes'
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
indexOfnumericvar <- c(6, 19, 20)
dfcat <- df[ , -indexOfnumericvar]
dfnum <- df[ , c(indexOfnumericvar, 21)]
index <- c(1, 7, 8, 20)
data <- df[ , -index]
model <- glm(Churn~., family = binomial(link = "logit"), data = data)
model
chisq <- data.frame(chi2statistic = 0, df = 0, p_value = 0, vcramer = 0)
for (i in dfcat[ , -1]) {
test <- chisq.test(i, dfcat$Churn)
# vcramer = xi2/(min(f-1,c-1))*numobs y aquí el mínimo siempre es 1, por churn
vcramer <- sqrt(test$statistic/nrow(dfcat))
newrow <- as.numeric(c(test$statistic, test$parameter, test$p.value, vcramer))
chisq <- rbind(chisq, newrow)
}
chisq <- chisq[-1, ]
chisq$p_value <- round(chisq$p_value, 3)
chisq$vcramer <- round(chisq$vcramer, 2)
rownames(chisq) <- names(dfcat[ , -1])
chisq
print(chisq)
df$Churn[df$OnlineSecurity=='No internet service', ]
df$Churn[df$OnlineSecurity=='No internet service']
df$InternetService
sum(df$InternetService=='No')
names(data)
index <- c(1, 2, 7, 8, 20)
data <- df[ , -index]
names(data)
model <- glm(Churn~SeniorCitizen+Partner+Dependents+tenure+InternetService+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges, family = binomial(link = "logit"), data = data)
model
model$R
model$contrasts
model$effects
model$rank
model$qr
exp(model$coefficients)
1/exp(model$coefficients)
pam_fit <- pam(dissmatrix, diss = TRUE, k = 2)
# Carga de paquetes
library(tables)
library(Hmisc)
library(party)
library(rpart)
library(rpart.plot)
library(randomForest)
library(htmlTable)
library(cluster)
# Carga y manejo de datos
file <- "Telco-Customer-Churn.csv"
df <- read.csv(file = file, header = T, sep = ",", na.strings = "NA")
setwd("~/ACUE/Ejercicio1")
df <- read.csv(file = file, header = T, sep = ",", na.strings = "NA")
df <- df[ , -c(1,20)]
df <- na.omit(df)
df$SeniorCitizen[df$SeniorCitizen == 0] <- 'No'
df$SeniorCitizen[df$SeniorCitizen == 1] <- 'Yes'
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
dfyes <- df[df$Churn == 'Yes' , ]
dfno  <- df[df$Churn == 'No'  , ]
# estandarizamos las variables numéricas
dissmatrix <- daisy(dfno, metric = 'gower')
pam_fit <- pam(dissmatrix, diss = TRUE, k = 2)
clusplot(pam_fit)
memory.limit()
